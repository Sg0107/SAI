{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ldJQDMmydFlE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from pre_process import pre_processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CuLujysdX6r",
        "outputId": "08003456-450e-4e89-ecf1-abc6668aeb43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Shubham Garg\\Documents\\Internship@SAI\\Day1\\pre_process.py:43: RuntimeWarning: invalid value encountered in cast\n",
            "  return buy_sequences.astype(np.int32), attention_masks.astype(np.int32), targets.astype(np.int32)\n"
          ]
        }
      ],
      "source": [
        "sequence_length = 20\n",
        "X, a, y = pre_processing('1_PnL.csv','with_Indicator_1h_BTC_USDT.csv', sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u_GU3hFSdbKO"
      },
      "outputs": [],
      "source": [
        "X_train = X[2:1100]\n",
        "y_train = y[2:1100]\n",
        "a_train = a[2:1100]\n",
        "X_test = X[1100:1200]\n",
        "y_test = y[1100:1200]\n",
        "a_test = a[1100:1200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQjtbJLloCGS",
        "outputId": "10935214-fcd8-415f-ada6-6280b1666b2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  1,   2,   1,   1,   1,   1,  65,   1,   1,   1,   5,   1,   3,\n",
              "         2,   1,   1,  66,   2,   1,   2,   5,   9,   8,   4,   1,   1,\n",
              "        66,   3,   2,   3,   8,   5,   8,   7,   1,   1,  66,   5,   4,\n",
              "         5,  11,  98,  14,  99,  28,   7,  26,  26,  16,  11,  99, 100,\n",
              "        91,  91,  42,  13,  12,  40,  26,  18,  91,  93,  99,  97,  54,\n",
              "        19,   4,  53,  36,  24,  96,  94,  88,  85,  62,  25,   1,  61,\n",
              "        43,  30,  87,  81,  82,  80,  68,  31,   2,  67,  50,  36,  81,\n",
              "        82,  88,  82,  74,  37,   4,  71,  55,  41,  82,  91,  90,  90,\n",
              "        80,  43,   7,  77,  62,  47,  90,  95,  98, 100,  86,  50,  11,\n",
              "        83,  68,  53, 100,  92,  91,  88,  90,  56,  17,  86,  73,  59,\n",
              "        88,  81,  90,  90,  93,  62,  26,  88,  78,  65,  90,  86,  96,\n",
              "        92,  95,  69,  36,  91,  82,  71,  92,  85,  98,  90,  97,  75,\n",
              "        47,  92,  86,  76,  90,  85,  95,  93,  98,  82,  60,  94,  89,\n",
              "        82,  93,  88,  96,  91,  99,  88,  72,  95,  93,  88,  91,  88,\n",
              "        99,  96, 100,  93,  83,  97,  96,  94,  96,  94, 100,  99,  99,\n",
              "       100, 100, 100, 100, 100], dtype=int32)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rL7jayyDddcg"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {'input_ids': X_train, 'attention_mask': a_train},\n",
        "    y_train\n",
        ")).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {'input_ids': X_test, 'attention_mask': a_test},\n",
        "    y_test\n",
        ")).batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9LZfaH0dfYX",
        "outputId": "0d60a76b-5112-42b9-f046-4f985c7deb8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, AdamWeightDecay\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUfQKnp1DnBe",
        "outputId": "d5c12487-ef50-4328-8ae6-9c1aea107171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "35/35 [==============================] - 33s 698ms/step - loss: 0.8866 - accuracy: 0.6630 - val_loss: 0.8159 - val_accuracy: 0.7000\n",
            "Epoch 2/10\n",
            "35/35 [==============================] - 23s 648ms/step - loss: 0.8447 - accuracy: 0.6785 - val_loss: 0.8046 - val_accuracy: 0.7000\n",
            "Epoch 3/10\n",
            "35/35 [==============================] - 23s 659ms/step - loss: 0.8385 - accuracy: 0.6785 - val_loss: 0.8057 - val_accuracy: 0.7000\n",
            "Epoch 4/10\n",
            "35/35 [==============================] - 23s 652ms/step - loss: 0.8399 - accuracy: 0.6785 - val_loss: 0.8050 - val_accuracy: 0.7000\n",
            "Epoch 5/10\n",
            "35/35 [==============================] - 22s 636ms/step - loss: 0.8387 - accuracy: 0.6785 - val_loss: 0.8034 - val_accuracy: 0.7000\n",
            "Epoch 6/10\n",
            "35/35 [==============================] - 23s 656ms/step - loss: 0.8365 - accuracy: 0.6785 - val_loss: 0.8043 - val_accuracy: 0.7000\n",
            "Epoch 7/10\n",
            "35/35 [==============================] - 22s 637ms/step - loss: 0.8367 - accuracy: 0.6785 - val_loss: 0.8039 - val_accuracy: 0.7000\n",
            "Epoch 8/10\n",
            "35/35 [==============================] - 23s 655ms/step - loss: 0.8383 - accuracy: 0.6785 - val_loss: 0.8055 - val_accuracy: 0.7000\n",
            "Epoch 9/10\n",
            "35/35 [==============================] - 22s 638ms/step - loss: 0.8369 - accuracy: 0.6785 - val_loss: 0.8046 - val_accuracy: 0.7000\n",
            "Epoch 10/10\n",
            "35/35 [==============================] - 23s 657ms/step - loss: 0.8378 - accuracy: 0.6785 - val_loss: 0.8041 - val_accuracy: 0.7000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7c644f731330>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define optimizer\n",
        "optimizer = AdamWeightDecay(learning_rate=1e-5, weight_decay_rate=0.01)\n",
        "\n",
        "# Compile the model, explicitly setting 'from_logits=True'\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Explicitly use SparseCategoricalCrossentropy and set from_logits=True\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=test_dataset,\n",
        "    steps_per_epoch=len(train_dataset),\n",
        "    validation_steps=len(test_dataset)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3RarFnePqXx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "shubham_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
